{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Headline Scraping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOV6CRHMxFzc3O+3wF6XXtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaBujniewicz/Masters_thesis/blob/main/4_Headline_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmY6xnHFHOSv"
      },
      "source": [
        "# this code  was executed in visual studio code and is posted to colab for clarity's sake\n",
        "# importing the needed packages\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import csv\n",
        "import newspaper\n",
        "\n",
        "# definig the function that gets the article titles (using newspaper3k)\n",
        "def get_titles(url):\n",
        "    try:\n",
        "        article = newspaper.Article(url) \n",
        "        article.download() \n",
        "        article.parse() \n",
        "        return article.title\n",
        "# this prevents the function from throwing an error when a link has been removed\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "# here any of the csv files can be inserted\n",
        "#FILE NAME HERE\n",
        "csv_file = 'amzn_data.csv'\n",
        "\n",
        "# importing the file in a bit of a weird way\n",
        "# the csv file is space delimeted, but spaces are also present within the tweets, so pandas can't be used\n",
        "# instead taking first 5 coma separated words as first 5 columns and everything that remains as the 6th one\n",
        "with open(csv_file, 'r+', encoding='utf-8') as f:\n",
        "    data = []\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        try:\n",
        "            text = line.split(' ')\n",
        "            data.append([text[0], text[1], text[2], text[3], text[4], ' '.join(text[5:])])\n",
        "        except:\n",
        "            data.append([None, None, None, None, None, None])\n",
        "                              \n",
        "    # regular expression pattern for identifying any url\n",
        "    regex = r\"http\\S+\"\n",
        "\n",
        "    # finding all urls within the tweets and appending them to the dataset\n",
        "    for i, tweet in enumerate(data):\n",
        "        text = str(tweet[5])\n",
        "        links = re.findall(regex, text)\n",
        "        data[i].append(links)\n",
        "\n",
        "    # creating a dataframe, only selecting the column with the date and a link\n",
        "    data = pd.DataFrame(data)\n",
        "    data = data.loc[:,[1,2,6]]\n",
        "    data.rename(columns={1: 'date', 2: 'time', 6: 'link'},inplace=True)\n",
        "\n",
        "    # filtering out links after the first day of february\n",
        "    data[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y/%m/%d\", errors = \"coerce\")\n",
        "    data = data[data[\"date\"]<\"2021-02-01\"]\n",
        "\n",
        "    # in case multiple links were found, putting them in separate rows with the same date\n",
        "    data = pd.DataFrame({\n",
        "            col:np.repeat(data[col].values, data[\"link\"].str.len())\n",
        "            for col in data.columns.drop(\"link\")}\n",
        "            ).assign(**{\"link\":np.concatenate(data[\"link\"].values)})[data.columns]\n",
        "    \n",
        "    # dropping any duplicate links\n",
        "    data.drop_duplicates(subset = \"link\", inplace=True)\n",
        "    # applying the function that finds article titles to all the links\n",
        "    data[\"title\"] = data[\"link\"].map(lambda x: get_titles(x))\n",
        "    # dropping the missing values\n",
        "    data.dropna(inplace=True)\n",
        "    #only keeping the titles longer than 5 words\n",
        "    data = data[data['title'].str.split().str.len().ge(5)]\n",
        "    #dropping any duplicate headlines\n",
        "    data.drop_duplicates(subset = \"title\", inplace=True)\n",
        "    # saving the file\n",
        "    data.to_csv(\"proc_\"+csv_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}